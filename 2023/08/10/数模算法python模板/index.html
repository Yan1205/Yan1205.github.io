<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>数模算法python模板 | 风雨天一阁📜</title><meta name="author" content="Yan1205"><meta name="copyright" content="Yan1205"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="基础的数模算法模板"><link rel="shortcut icon" href="/img/1205.png"><link rel="canonical" href="http://yan1205.top/2023/08/10/%E6%95%B0%E6%A8%A1%E7%AE%97%E6%B3%95python%E6%A8%A1%E6%9D%BF/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"top-right"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '数模算法python模板',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-09-09 23:27:02'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"><span id="fps"></span><link rel="stylesheet" href="//at.alicdn.com/t/c/font_3865131_23yk0m7309o.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="/css/cat.css"><link rel="stylesheet" href="/css/readPercent.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/css/macblack.css"><div id="myscoll"></div><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload="this.media='screen'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/head3.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">19</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/"><i class="fa-fw icon-home"></i><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-home"></use></svg><span> 首页</span></a></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-he_00-qingdanliebiao"></use></svg><span> 书肆</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-TIME">                   </use></svg><span> 时间轴</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-fenlei2">                   </use></svg><span> 分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-biaoqian">                   </use></svg><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-baoxiang2"></use></svg><span> 宝藏</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/movies/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-professional-movie-c">                   </use></svg><span> 剧院</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/music/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-Music">                   </use></svg><span> 音乐</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/images/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-tubiaozhizuomoban">                   </use></svg><span> 相簿</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/love/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-xin">                   </use></svg><span> 情书</span></a></li></ul></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/workplace/"><i class="fa-fw icon-workspace"></i><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-workspace"></use></svg><span> 工作台</span></a></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-A"></use></svg><span> 足迹</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/comments/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-liuyanmoban">                   </use></svg><span> 留言</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/link/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-lianjie">                   </use></svg><span> 友链</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/visitor/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-tubiaozhizuomoban1">                   </use></svg><span> 访客</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/about/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-roundabout">                   </use></svg><span> 关于</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.skypro.cartafi.cn/2023/04/24/644563850bd39.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="风雨天一阁📜"><span class="site-name">风雨天一阁📜</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i></a></div><div class="menus_items"><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/"><i class="fa-fw icon-home"></i><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-home"></use></svg><span> 首页</span></a></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-he_00-qingdanliebiao"></use></svg><span> 书肆</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-TIME">                   </use></svg><span> 时间轴</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-fenlei2">                   </use></svg><span> 分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-biaoqian">                   </use></svg><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-baoxiang2"></use></svg><span> 宝藏</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/movies/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-professional-movie-c">                   </use></svg><span> 剧院</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/music/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-Music">                   </use></svg><span> 音乐</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/images/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-tubiaozhizuomoban">                   </use></svg><span> 相簿</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/love/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-xin">                   </use></svg><span> 情书</span></a></li></ul></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/workplace/"><i class="fa-fw icon-workspace"></i><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-workspace"></use></svg><span> 工作台</span></a></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-A"></use></svg><span> 足迹</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/comments/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-liuyanmoban">                   </use></svg><span> 留言</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/link/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-lianjie">                   </use></svg><span> 友链</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/visitor/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-tubiaozhizuomoban1">                   </use></svg><span> 访客</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/about/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-roundabout">                   </use></svg><span> 关于</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">数模算法python模板</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-08-09T18:25:37.000Z" title="发表于 2023-08-10 02:25:37">2023-08-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-09-09T15:27:02.042Z" title="更新于 2023-09-09 23:27:02">2023-09-09</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%95%B0%E5%AD%A6/">数学</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">8.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>36分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="数模算法python模板"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="一、规划问题"><a href="#一、规划问题" class="headerlink" title="一、规划问题"></a>一、规划问题</h1><h2 id="1-线性规划"><a href="#1-线性规划" class="headerlink" title="(1)线性规划"></a>(1)线性规划</h2><ol>
<li>pulp解决简单线性规划问题</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pulp</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.定义一个规划问题</span></span><br><span class="line">MyProbLP = pulp.LpProblem(<span class="string">&quot;LPProbDemo1&quot;</span>, sense=pulp.LpMaximize)	<span class="comment">#LpMinimize/LpMaximize</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.定义决策变量</span></span><br><span class="line">x1 = pulp.LpVariable(<span class="string">&#x27;x1&#x27;</span>, lowBound=<span class="number">0</span>, upBound=<span class="number">7</span>, cat=<span class="string">&#x27;Continuous&#x27;</span>)	<span class="comment">#Integer/Binary/Continuous</span></span><br><span class="line">x2 = pulp.LpVariable(<span class="string">&#x27;x2&#x27;</span>, lowBound=<span class="number">0</span>, upBound=<span class="number">7</span>, cat=<span class="string">&#x27;Continuous&#x27;</span>)</span><br><span class="line">x3 = pulp.LpVariable(<span class="string">&#x27;x3&#x27;</span>, lowBound=<span class="number">0</span>, upBound=<span class="number">7</span>, cat=<span class="string">&#x27;Continuous&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.设置目标函数</span></span><br><span class="line">MyProbLP += <span class="number">2</span> * x1 + <span class="number">3</span> * x2 - <span class="number">5</span> * x3</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.添加约束条件</span></span><br><span class="line">MyProbLP += (<span class="number">2</span> * x1 - <span class="number">5</span> * x2 + x3 &gt;= <span class="number">10</span>)  <span class="comment"># 不等式约束</span></span><br><span class="line">MyProbLP += (x1 + <span class="number">3</span> * x2 + x3 &lt;= <span class="number">12</span>)  <span class="comment"># 不等式约束</span></span><br><span class="line">MyProbLP += (x1 + x2 + x3 == <span class="number">7</span>)  <span class="comment"># 等式约束</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.求解</span></span><br><span class="line">MyProbLP.solve()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Status:&quot;</span>, pulp.LpStatus[MyProbLP.status])  <span class="comment"># 输出求解状态</span></span><br><span class="line"><span class="keyword">for</span> v <span class="keyword">in</span> MyProbLP.variables():</span><br><span class="line">    <span class="built_in">print</span>(v.name, <span class="string">&quot;=&quot;</span>, v.varValue)  <span class="comment"># 输出每个变量的最优值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;F(x) = &quot;</span>, pulp.value(MyProbLP.objective))  <span class="comment"># 输出最优解的目标函数值</span></span><br></pre></td></tr></table></figure>
<ol>
<li><p>scipy求解</p>
<p>A，Aeq是二维数组，其他是一维数组。</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> optimi ze</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">c=np.array([<span class="number">2</span>,<span class="number">3</span>,-<span class="number">5</span>])</span><br><span class="line">A=np.array([[-<span class="number">2</span>,<span class="number">5</span>,-<span class="number">1</span>],[<span class="number">1</span>,<span class="number">3</span>,<span class="number">1</span>]])	</span><br><span class="line">B=np.array([-<span class="number">10</span>,<span class="number">12</span>])</span><br><span class="line">Aeq=np.array([[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line">Beq=np.array([<span class="number">7</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#求解函数</span></span><br><span class="line">res =optimize.linprog (C, A,b, Aeq, beq, LB, UB, X0, OPTIONS)</span><br><span class="line"><span class="built_in">print</span> (res)</span><br></pre></td></tr></table></figure>
<p>摘录：</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_46692607/article/details/126784109">线性规划和例题</a></p>
<h1 id="二、相关性分析"><a href="#二、相关性分析" class="headerlink" title="二、相关性分析"></a>二、相关性分析</h1><p>其实用Excel也能做:</p>
<figure class="highlight vb"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">=PEARSON(A2:A33)</span><br></pre></td></tr></table></figure>
<p>用python生成热力图:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">df=pd.read_excel(<span class="string">&#x27;附件1&#x27;</span>)</span><br><span class="line">df</span><br><span class="line">df.corr(<span class="string">&quot;pearson&quot;</span>)</span><br><span class="line">df.corr(<span class="string">&quot;spearman&quot;</span>)</span><br><span class="line">df.corr(<span class="string">&quot;kendall&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>]=<span class="string">&#x27;Simhei&#x27;</span>	<span class="comment">#修改字体</span></span><br><span class="line">ax=sns.heatmap(</span><br><span class="line">  df.corr(<span class="string">&quot;pearson&quot;</span>),</span><br><span class="line">  annot=<span class="literal">True</span>,</span><br><span class="line">  cmap=<span class="string">&quot;coolwarm&quot;</span>,</span><br><span class="line">  fmt=<span class="string">&#x27;.2f&#x27;</span></span><br><span class="line">)</span><br><span class="line">bottom,top=ax.get_ylim()</span><br><span class="line">ax.set_ylim(bottom+<span class="number">0.5</span>,top-<span class="number">0.5</span>)</span><br><span class="line">plt.savfig()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> spearmanr</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置中文字体</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>]</span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据加载</span></span><br><span class="line">data = pd.read_excel(<span class="string">&#x27;use_for_py.xlsx&#x27;</span>)</span><br><span class="line">data_analysis = data.head(<span class="number">391</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算相关系数矩阵</span></span><br><span class="line">correlation_matrix = data_analysis.corr(method=<span class="string">&#x27;spearman&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制热力图</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">sns.heatmap(correlation_matrix, annot=<span class="literal">True</span>, cmap=<span class="string">&#x27;coolwarm&#x27;</span>, linewidths=<span class="number">0.5</span>, fmt=<span class="string">&quot;.2f&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Spearman Correlation Heatmap&#x27;</span>)</span><br><span class="line">plt.xticks(rotation=<span class="number">45</span>)</span><br><span class="line">plt.yticks(rotation=<span class="number">0</span>)</span><br><span class="line">plt.savefig(<span class="string">&#x27;Corr_heat.png&#x27;</span>, dpi=<span class="number">300</span>, bbox_inches=<span class="string">&#x27;tight&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>Pearson相关分析:（p∈[-1,1]）</p>
<p>$\rho<em>{X, Y} = \frac{\operatorname{cov}(X, Y)}{\sigma</em>{X} \sigma<em>{Y}} = \frac{E\left(\left(X-\mu</em>{X}\right)\left(Y-\mu<em>{Y}\right)\right)}{\sigma</em>{X} \sigma<em>{Y}} = \frac{E(X Y)-E(X) E(Y)}{\sqrt{E\left(X^{2}\right)-E^{2}(X)} \sqrt{E\left(Y^{2}\right)-E^{2}(Y)}} \\rho</em>{X, Y} = \frac{N \sum X Y-\sum X \sum Y}{\sqrt{N \sum X^{2}-\left(\sum X\right)^{2}} \sqrt{N \sum Y^{2}-\left(\sum Y\right)^{2}}} \\rho_{X, Y} = \frac{\sum X Y-\frac{\sum X \sum Y}{N}}{\sqrt{\left(\sum X^{2}-\frac{\left(\sum X\right)^{2}}{N}\right)\left(\sum Y^{2}-\frac{\left(\sum Y\right)^{2}}{N}\right)}}$</p>
<h2 id="t-检验：检验两个变量是否存在差异"><a href="#t-检验：检验两个变量是否存在差异" class="headerlink" title="t-检验：检验两个变量是否存在差异"></a>t-检验：检验两个变量是否存在差异</h2><p>如果满足两个假设：</p>
<p>​    ①正态分布：</p>
<p>​        被测量的变量需要在总体和样本中呈现正态分布；    </p>
<p>​        即使不满足，根据中心极限定理，若每组样本大于30时，均值分布趋近于正态分布。</p>
<p>​    ②方差齐性：</p>
<p>​        需要两样本之间的方差不能差太多。</p>
<p>可以用T检验(t-test)，计算T值，若大于临界值，则能说明有差异性。</p>
<p>此外，还可以考虑用 配对样本t检验(Dependent t-test for paired samples) 和 单样本t检验(One-sample t-test)。</p>
<p>摘录</p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV13h411872K/?spm_id_from=333.337.search-card.all.click&amp;vd_source=15aaf2f3032abe28c98a76535d8c050b">相关性分析-简单讲解</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1eg411a7eh/?spm_id_from=333.788&amp;vd_source=15aaf2f3032abe28c98a76535d8c050b">T-检验等统计方法速览</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV14Y4y1t7Yj/?spm_id_from=333.337.search-card.all.click&amp;vd_source=15aaf2f3032abe28c98a76535d8c050b">spsspro相关性分析</a></p>
<h1 id="三、数据处理和机器学习"><a href="#三、数据处理和机器学习" class="headerlink" title="三、数据处理和机器学习"></a>三、数据处理和机器学习</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> read_csv</span><br><span class="line"><span class="keyword">from</span> pandas.plotting <span class="keyword">import</span> scatter_matrix</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line"><span class="comment">#导入数据集</span></span><br><span class="line">filename = <span class="string">&#x27;C:/Users/Lenovo/Desktop/上课/python机器学习课程/5.13/iris.data.csv&#x27;</span></span><br><span class="line">names = [<span class="string">&#x27;separ-length&#x27;</span>, <span class="string">&#x27;separ-width&#x27;</span>, <span class="string">&#x27;petal-length&#x27;</span>, <span class="string">&#x27;patal-width&#x27;</span>, <span class="string">&#x27;class&#x27;</span>]</span><br><span class="line">dateset = read_csv(filename, names=names)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;数据维度：行 %s, 列 %s&#x27;</span> % dateset.shape)	<span class="comment">#行&amp;列</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#查看一下数据集</span></span><br><span class="line"><span class="built_in">print</span>(dateset.head(<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#描述性统计</span></span><br><span class="line"><span class="built_in">print</span>(dateset.describe())</span><br><span class="line"><span class="comment">#数据分布情况</span></span><br><span class="line"><span class="built_in">print</span>(dateset.groupby(<span class="string">&#x27;class&#x27;</span>).size())</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据可视化</span></span><br><span class="line">dateset.plot(kind=<span class="string">&#x27;box&#x27;</span>, subplots=<span class="literal">True</span>, layout=(<span class="number">2</span>,<span class="number">2</span>), sharex=<span class="literal">False</span>, sharey=<span class="literal">False</span>)</span><br><span class="line">pyplot.show()	<span class="comment">#箱型图</span></span><br><span class="line"></span><br><span class="line">dateset.hist()</span><br><span class="line">pyplot.show()	<span class="comment">#直方图</span></span><br><span class="line"></span><br><span class="line">scatter_matrix(dateset)</span><br><span class="line">pyplot.show()	<span class="comment">#散点矩阵图</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#分离数据集</span></span><br><span class="line">array = dateset.values</span><br><span class="line">X = array[:, <span class="number">0</span>:<span class="number">4</span>]</span><br><span class="line">Y = array[:, <span class="number">4</span>]</span><br><span class="line">validation_size = <span class="number">0.2</span></span><br><span class="line">seed = <span class="number">7</span></span><br><span class="line">X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size, random_state=seed, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#算法</span></span><br><span class="line">models = &#123;&#125;</span><br><span class="line">models[<span class="string">&#x27;LR&#x27;</span>] = LogisticRegression()</span><br><span class="line">models[<span class="string">&#x27;KNN&#x27;</span>] = KNeighborsClassifier()</span><br><span class="line">models[<span class="string">&#x27;LDA&#x27;</span>] = LinearDiscriminantAnalysis()</span><br><span class="line">models[<span class="string">&#x27;CART&#x27;</span>] = DecisionTreeClassifier()</span><br><span class="line">models[<span class="string">&#x27;SVM&#x27;</span>] = SVC()</span><br><span class="line">models[<span class="string">&#x27;NB&#x27;</span>] = GaussianNB()</span><br><span class="line"></span><br><span class="line">results = []</span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> models:</span><br><span class="line">    kflod = KFold(n_splits=<span class="number">10</span>, random_state=seed, shuffle=<span class="literal">True</span>)</span><br><span class="line">    cv_results = cross_val_score(models[key], X_train, Y_train, cv=kflod, scoring=<span class="string">&#x27;accuracy&#x27;</span>)</span><br><span class="line">    results.append(cv_results)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;%s: %f (%f)&#x27;</span> %(key, cv_results.mean(), cv_results.std()))</span><br><span class="line"></span><br><span class="line">svm = SVC()</span><br><span class="line">svm.fit(X=X_train, y=Y_train)</span><br><span class="line">predictions = svm.predict(X_validation)</span><br><span class="line"><span class="built_in">print</span>(accuracy_score(Y_validation, predictions))</span><br><span class="line"><span class="built_in">print</span>(confusion_matrix(Y_validation, predictions))</span><br><span class="line"><span class="built_in">print</span>(classification_report(Y_validation, predictions))</span><br></pre></td></tr></table></figure>
<h2 id="数据导入-三种方式"><a href="#数据导入-三种方式" class="headerlink" title="数据导入#三种方式"></a>数据导入#三种方式</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#csv特征：，分隔 文件头：字段属性</span></span><br><span class="line"><span class="keyword">from</span> csv <span class="keyword">import</span> reader</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">filename = <span class="string">&#x27;pima_data.csv&#x27;</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&#x27;rt&#x27;</span>) <span class="keyword">as</span> raw_data:</span><br><span class="line">    readers = reader(raw_data, delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">    x = <span class="built_in">list</span>(readers)</span><br><span class="line">    data = np.array(x).astype(<span class="string">&#x27;float&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Pandas导入</span></span><br><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> read_csv</span><br><span class="line">filename = <span class="string">&#x27;pima_data.csv&#x27;</span></span><br><span class="line">names = [<span class="string">&#x27;preg&#x27;</span>, <span class="string">&#x27;plas&#x27;</span>, <span class="string">&#x27;pres&#x27;</span>, <span class="string">&#x27;skin&#x27;</span>, <span class="string">&#x27;test&#x27;</span>, <span class="string">&#x27;mass&#x27;</span>, <span class="string">&#x27;pedi&#x27;</span>, <span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;class&#x27;</span>]</span><br><span class="line">data = read_csv(filename, names=names)</span><br><span class="line"><span class="built_in">print</span>(data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#numpy导入</span></span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> loadtxt</span><br><span class="line">filename = <span class="string">&#x27;pima_data.csv&#x27;</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&#x27;rt&#x27;</span>) <span class="keyword">as</span> raw_data:</span><br><span class="line">    data = loadtxt(raw_data, delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(data.shape)</span><br></pre></td></tr></table></figure>
<h2 id="数据理解"><a href="#数据理解" class="headerlink" title="数据理解"></a>数据理解</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(data,head(<span class="number">10</span>))</span><br><span class="line"><span class="comment">#维度</span></span><br><span class="line"><span class="built_in">print</span>(data.shape)</span><br><span class="line"><span class="comment">#属性和类型</span></span><br><span class="line"><span class="built_in">print</span>(data.dtypes)</span><br><span class="line"><span class="comment">#描述统计，放入论文</span></span><br><span class="line"><span class="built_in">print</span>(data.describe())</span><br><span class="line"><span class="comment">#分布,用于分类算法</span></span><br><span class="line"><span class="built_in">print</span>(data.groupby(<span class="string">&#x27;class&#x27;</span>).size())</span><br><span class="line"><span class="comment">#相关性:&#x27;pearson&#x27; &#x27;spearman&#x27; &#x27;kendall&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(data.corr(method=<span class="string">&#x27;pearson&#x27;</span>))</span><br><span class="line"><span class="comment">#高斯分布</span></span><br><span class="line"><span class="built_in">print</span>(data.skew())</span><br></pre></td></tr></table></figure>
<h2 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">data.hist()	<span class="comment">#直方图</span></span><br><span class="line">data.plot(kind=<span class="string">&#x27;desnity&#x27;</span>, subplots=<span class="literal">True</span>, layout(<span class="number">3</span>,<span class="number">3</span>), shareX=<span class="literal">False</span>)	<span class="comment">#密度图</span></span><br><span class="line">data.plot(kind=<span class="string">&#x27;box&#x27;</span>, subplots=<span class="literal">True</span>, layout(<span class="number">3</span>,<span class="number">3</span>), shareX=<span class="literal">False</span>)	<span class="comment">#箱型图</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#散点矩阵图</span></span><br><span class="line">correlations = data.corr()</span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">cax = ax.matshow(correlations, vmin=-<span class="number">1</span>, vmax=<span class="number">1</span>)</span><br><span class="line">fig.colorbar(cax)</span><br><span class="line">ticks = np.arange(<span class="number">0</span>,<span class="number">9</span>,<span class="number">1</span>)</span><br><span class="line">ax.set_xticks(ticks)</span><br><span class="line">ax.set_yticks(ticks)</span><br><span class="line">ax.set_xticklabels(names)</span><br><span class="line">ax.set_yticklabels(names)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#散点图</span></span><br><span class="line">data = read_csv(filename, names=names)</span><br><span class="line">scatter_matrix(data)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="数据预处理-将数据转换到-0-1"><a href="#数据预处理-将数据转换到-0-1" class="headerlink" title="数据预处理:将数据转换到[0,1]."></a>数据预处理:将数据转换到[0,1].</h2><h3 id="①数据缩放（-MinMaxScaler-）"><a href="#①数据缩放（-MinMaxScaler-）" class="headerlink" title="①数据缩放（ MinMaxScaler ）"></a>①数据缩放（ MinMaxScaler ）</h3><p>​    使用算法： K近邻算法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> set_printoptions</span><br><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> read_csv</span><br><span class="line"></span><br><span class="line">data = read_csv(filename, names=names)</span><br><span class="line">array = data.values</span><br><span class="line">X = array[:, <span class="number">0</span>:<span class="number">8</span>]</span><br><span class="line">Y = array[:, <span class="number">8</span>]</span><br><span class="line">transformer = MinMaxScaler(feature_range=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">newX = transformer.fit_transform(X)</span><br><span class="line">set_printoptions(precision=<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(newX)</span><br></pre></td></tr></table></figure>
<h3 id="②正态化数据（StandardScaler）"><a href="#②正态化数据（StandardScaler）" class="headerlink" title="②正态化数据（StandardScaler）"></a>②正态化数据（StandardScaler）</h3><p>​     使用算法：线性回归、逻辑回归、判别分析</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line">transformer = StandardScaler().fit(X)</span><br><span class="line">newX = transformer.transform(X)</span><br><span class="line">set_printoptions(precision=<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(newX)</span><br></pre></td></tr></table></figure>
<h3 id="③标准化数据（Normalizer）常用"><a href="#③标准化数据（Normalizer）常用" class="headerlink" title="③标准化数据（Normalizer）常用"></a>③标准化数据（Normalizer）<strong>常用</strong></h3><p>​    使用模型：神经网络、K近邻算法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Normalizer</span><br><span class="line"></span><br><span class="line">transformer = Normalizer().fit(X)</span><br><span class="line">newX = transformer.transform(X)</span><br><span class="line">set_printoptions(precision=<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(newX)</span><br></pre></td></tr></table></figure>
<h3 id="④二值数据（Binarizer）"><a href="#④二值数据（Binarizer）" class="headerlink" title="④二值数据（Binarizer）"></a>④二值数据（Binarizer）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Binarizer</span><br><span class="line"></span><br><span class="line">transformer = Binarizer(threshold=<span class="number">0.0</span>).fit(X)</span><br><span class="line">newX = transformer.transform(X)</span><br><span class="line">set_printoptions(precision=<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(newX)</span><br></pre></td></tr></table></figure>
<h2 id="数据特征选定"><a href="#数据特征选定" class="headerlink" title="数据特征选定"></a>数据特征选定</h2><h3 id="①单变量特征选定-SelectKBest类"><a href="#①单变量特征选定-SelectKBest类" class="headerlink" title="①单变量特征选定(SelectKBest类)"></a>①单变量特征选定(SelectKBest类)</h3><p>​    理论：经典的卡方检验是检验定性自变量对定性自变量的相关性的方法。</p>
<p>​    多用于检验</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> set_printoptions</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</span><br><span class="line"></span><br><span class="line">test = SelectKBest(score_func=chi2, k=<span class="number">4</span>)</span><br><span class="line">fit = test.fit(X, Y)</span><br><span class="line">set_printoptions(precision=<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(fit.scores_)</span><br><span class="line">features = fit.transform(X)</span><br><span class="line"><span class="built_in">print</span>(features)</span><br></pre></td></tr></table></figure>
<h3 id="②递归特征消除-常用"><a href="#②递归特征消除-常用" class="headerlink" title="②递归特征消除    常用"></a>②递归特征消除    <strong>常用</strong></h3><p>​    理论：用基模型筛选特征</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#逻辑回归为例</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFE</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line">model = LogisticRegression()</span><br><span class="line">rfe = RFE(model, n_features_to_select=<span class="number">3</span>)</span><br><span class="line">fit = rfe.fit(X,Y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;特征个数:&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(fit.n_features_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;被选特个数:&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(fit.support_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;特征排名:&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(fit.ranking_)</span><br></pre></td></tr></table></figure>
<h3 id="③主成分分析"><a href="#③主成分分析" class="headerlink" title="③主成分分析"></a>③主成分分析</h3><p>​    理论：主成分分析(PCA) 是使用线性代数来转换压缩数据，通常被称作数据降维，常见的数据降维方法除了PCA(无监督的降维方法)，还有LDA (线性判别分析,有监督的降维方法)，它本身也是一个分类模型。</p>
<p>​    谨慎使用，合并后的新特征需要解释其意义。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"></span><br><span class="line">pca = PCA(n_components=<span class="number">3</span>)</span><br><span class="line">fit = pca.fit(X)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;解释方差:%s&#x27;</span> % fit.explained_variance_ratio_)</span><br><span class="line"><span class="built_in">print</span>(fit.components_)</span><br></pre></td></tr></table></figure>
<h3 id="④特征重要性"><a href="#④特征重要性" class="headerlink" title="④特征重要性"></a>④特征重要性</h3><p>​    理论：袋装随机树、随机森林、极端随机算法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> ExtraTreesClassifier</span><br><span class="line"></span><br><span class="line">model = ExtraTreesClassifier()</span><br><span class="line">fit = model.fit(X, Y)</span><br><span class="line"><span class="built_in">print</span>(fit.feature_importances_)</span><br></pre></td></tr></table></figure>
<h1 id="四、模型"><a href="#四、模型" class="headerlink" title="四、模型"></a>四、模型</h1><p>​    机器学习的解题步骤：</p>
<pre><code>定义问题：类库导入、数据导入；
理解数据：描述性统计、数据可视化；
数据准备：数据清洗、特征选取、数据转换；
评估算法：分离数据、定义模型评估标准、算法审查、算法比较；
优化模型：调参、集合算法；
结果部署：验证、生成。
</code></pre><h2 id="1-评估算法"><a href="#1-评估算法" class="headerlink" title="(1)评估算法"></a>(1)评估算法</h2><h3 id="①分离数据集和评估数据集"><a href="#①分离数据集和评估数据集" class="headerlink" title="①分离数据集和评估数据集"></a>①分离数据集和评估数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line">test_size = <span class="number">0.33</span></span><br><span class="line">seed = <span class="number">4</span></span><br><span class="line">X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,</span><br><span class="line">                                                    random_state=seed)</span><br><span class="line">model = LogisticRegression()</span><br><span class="line">model.fit(X_train, Y_train)</span><br><span class="line">result = model.score(X_test, Y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;算法评估结果： %.3f %%&#x27;</span> % (result * <span class="number">100</span>) )</span><br></pre></td></tr></table></figure>
<h3 id="②k折交叉验证分离"><a href="#②k折交叉验证分离" class="headerlink" title="②k折交叉验证分离"></a>②k折交叉验证分离</h3><p>​    为了提高模型准确率。</p>
<p>​    K折交叉验证是将原始数据分成K组（一般是均分），将每个子集数据分别做一次验证集，其余的K-1组子集数 据作为训练集，这样会得到K个模型，再用这K个模型最终的验证集的分类准确率的平均数， 作为此K折交叉验证下分类器的性能指标。K一般大于等于2，实际操作时一般从3开始取值， 只有在原始数据集和数据量小的时候才会尝试取2。K折交叉验证可以有效地避免过学习及欠学习状态的发生，最后得到的结果也比较具有说服力。通常情况下，K的取值为3、5、10.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br><span class="line">num_fold = <span class="number">10</span></span><br><span class="line">seed = <span class="number">7</span></span><br><span class="line">kfold = KFold(n_splits=num_fold, random_state=seed, shuffle=<span class="literal">True</span>)</span><br><span class="line">model = LogisticRegression(multi_class=<span class="string">&#x27;multinomial&#x27;</span>, max_iter=<span class="number">3000</span>)</span><br><span class="line">result = cross_val_score(model, X, Y, cv=kfold)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;算法结果： %.3f%% (%.3f%%)&quot;</span> % (result.mean() *<span class="number">100</span>, result.std() *<span class="number">100</span>))</span><br></pre></td></tr></table></figure>
<h3 id="③弃一交叉验证分离"><a href="#③弃一交叉验证分离" class="headerlink" title="③弃一交叉验证分离."></a>③弃一交叉验证分离.</h3><p>​    准确率高，但计算成本高。</p>
<p>​     如果原始数据有N个样本，那么弃一交叉验证就是N-1个交叉验证，即每个样本单独作为验证 集，其余的N-1个样本作为训练集，所以弃一交叉验证会得到N个模型，用这N个模型最终的 验证集的分类准确率的平均数作为此次弃一交叉验证分类器的性能指标。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> LeaveOneOut</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br><span class="line">loocv = LeaveOneOut()</span><br><span class="line">model = LogisticRegression(multi_class=<span class="string">&#x27;multinomial&#x27;</span>, max_iter=<span class="number">1100</span>)</span><br><span class="line">result = cross_val_score(model, X, Y, cv=loocv)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;算法评估：%.3f%% (%.3f%%)&quot;</span> % (result.mean()*<span class="number">100</span>, result.std()*<span class="number">100</span>))</span><br></pre></td></tr></table></figure>
<h3 id="④重复随机评估、训练数据集分离。"><a href="#④重复随机评估、训练数据集分离。" class="headerlink" title="④重复随机评估、训练数据集分离。"></a>④重复随机评估、训练数据集分离。</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> ShuffleSplit</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br><span class="line">n_splits = <span class="number">10</span></span><br><span class="line">test_size = <span class="number">0.33</span></span><br><span class="line">seed = <span class="number">7</span></span><br><span class="line">kfold = ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=seed)</span><br><span class="line">model = LogisticRegression(multi_class=<span class="string">&#x27;multinomial&#x27;</span>, max_iter=<span class="number">1100</span>)</span><br><span class="line">result = cross_val_score(model, X, Y, cv=kfold)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;算法评估：%.3f%% (%.3f%%)&quot;</span> % (result.mean()*<span class="number">100</span>, result.std()*<span class="number">100</span>))</span><br></pre></td></tr></table></figure>
<h2 id="2-算法评估"><a href="#2-算法评估" class="headerlink" title="(2)算法评估"></a>(2)算法评估</h2><p>​        寻找最佳的子集算法。重点工作在评估算法和准备数据上，要找到3-5种准确度足够的算法。</p>
<h3 id="①分类准确度"><a href="#①分类准确度" class="headerlink" title="①分类准确度"></a>①分类准确度</h3><p>​            分类准确度就是算法自动分类正确的样本数除以所有的样本数得出的结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">result = cross_val_score(model, X, Y, cv=kfold)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;算法评估：%.3f%% (%.3f%%)&quot;</span> % (result.mean()*<span class="number">100</span>, result.std()*<span class="number">100</span>))</span><br></pre></td></tr></table></figure>
<h3 id="②对数损失函数"><a href="#②对数损失函数" class="headerlink" title="②对数损失函数"></a>②对数损失函数</h3><p>​            分类准确度就是算法自动分类正确的样本数除以所有的样本数得出的结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">num_flods = <span class="number">10</span></span><br><span class="line">seed = <span class="number">7</span></span><br><span class="line">kfold = KFold(n_splits=num_flods, random_state=seed, shuffle=<span class="literal">True</span>)</span><br><span class="line">model = LogisticRegression(multi_class=<span class="string">&#x27;multinomial&#x27;</span>, max_iter=<span class="number">1100</span>)</span><br><span class="line"></span><br><span class="line">Scoring = <span class="string">&#x27;neg_log_loss&#x27;</span></span><br><span class="line">result = cross_val_score(model, X, Y, cv=kfold, scoring=Scoring)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;LogLoss: %.3f (%.3f) &#x27;</span> % (result.mean(), result.std()))</span><br></pre></td></tr></table></figure>
<h3 id="④AUC图"><a href="#④AUC图" class="headerlink" title="④AUC图"></a>④AUC图</h3><p>​            ROC和AUC是评价分类器的指标。ROC是受试者工作特征曲线(Receiver  OperatingCharacteristic Curve)的简写，又称为感受性曲线(Sensitivity Curve)。得此名的原 因在于曲线上各点反映相同的感受性，它们都是对同一信号刺激的反应，只不过是在几种不 同的判定标准下所得的结果而已。ROC是反映敏感性和特异性连续变量的综合指标，用构图 法揭示敏感性和特异性的相互关系，通过将连续变量设定出多个不同的临界值计算出一系列 敏感性和特异性，再以敏感性为纵坐标、(1-特异性)为横坐标绘制成曲线。AUC是ROC曲线 下的面积(Area Under ROC Curve)的简称，顾名思义，AUC的值就是处于ROC Curve下方的 那部分面积的大小。通常，AUC的值介于0.5到1.0之间，AUC的值越大，诊断准确性越高。 在ROC曲线上，靠近坐标图左上方的点为敏感性和特异性均较高的临界值。</p>
<p>​    召回率=TP/(TP+FN)，召回率（Recall）又叫敏感性（sensitivity）</p>
<p>​    特异度（Specificity） = TN/(FP+TN)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br><span class="line">scoring = <span class="string">&#x27;roc_auc&#x27;</span></span><br><span class="line">result = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;AUC %.3f (%.3f)&#x27;</span> %(result.mean(), result.std()))</span><br></pre></td></tr></table></figure>
<h3 id="⑤混淆矩阵"><a href="#⑤混淆矩阵" class="headerlink" title="⑤混淆矩阵"></a>⑤混淆矩阵</h3><p>​            混淆矩阵的每一列代表了预测类别，每一列的总数表示预测为该类别的数据的数目</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"></span><br><span class="line">test_size = <span class="number">0.33</span></span><br><span class="line">seed = <span class="number">4</span></span><br><span class="line">X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)</span><br><span class="line">model = LogisticRegression(multi_class=<span class="string">&#x27;multinomial&#x27;</span>, max_iter=<span class="number">1100</span>)</span><br><span class="line"></span><br><span class="line">model.fit(X_train, Y_train)</span><br><span class="line">predicted = model.predict(X_test)</span><br><span class="line">matrix = confusion_matrix(Y_test, predicted)</span><br><span class="line">classes = [<span class="string">&#x27;0&#x27;</span>, <span class="string">&#x27;1&#x27;</span>]</span><br><span class="line">dataframe = pd.DataFrame(data=matrix, index=classes, columns=classes)</span><br><span class="line"><span class="built_in">print</span>(dataframe)</span><br></pre></td></tr></table></figure>
<h3 id="⑥结果报告"><a href="#⑥结果报告" class="headerlink" title="⑥结果报告"></a>⑥结果报告</h3><p>​            精确率(precison)： P = TP / (TP + FP) </p>
<p>​            召回率(recall)： P = TP / (TP + FN) </p>
<p>​            F1值：同时兼顾了分类模型的精确率和召回率， F1分数可以看作是模型准确率和召 回率的一种加权平均，它的最大值是1，最小值是0，值越大意味着模型越好。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line">predicted = model.predict(X_test)</span><br><span class="line">report = classification_report(Y_test, predicted)</span><br><span class="line"><span class="built_in">print</span>(report)</span><br></pre></td></tr></table></figure>
<h3 id="⑦绝对均值误差"><a href="#⑦绝对均值误差" class="headerlink" title="⑦绝对均值误差"></a>⑦绝对均值误差</h3><p>​            </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scoring = <span class="string">&#x27;neg_mean_absolute_error&#x27;</span></span><br><span class="line">result = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;MAE: %.3f (%.3f)&#x27;</span> %(result.mean(), result.std()))</span><br></pre></td></tr></table></figure>
<h3 id="⑧决定系数（-𝑅-2-）"><a href="#⑧决定系数（-𝑅-2-）" class="headerlink" title="⑧决定系数（$𝑅^2$）"></a>⑧决定系数（$𝑅^2$）</h3><p>​    决定系数（$𝑅^2$） 决定系数，反映因变量的全部变异能通过回归关系被自变量解释的比例。拟合优度越大，自变 量对因变量的解释程度越高，自变量引起的变动占总变动的百分比越高，观察点在回归直线附 近越密集。如𝑅𝑅2为0.8，则表示回归关系可以解释因变量80%的变异。换句话说，如果我们能 控制自变量不变，则因变量的变异程度会减少80%。 决定系数（ 𝑅𝑅2 )的特点： 可决系数是非负的统计量。 可决系数的取值范围：0≤ 𝑅𝑅2 ≤1 可决系数是样本观测值的函数，是因随机抽样而变动的随机变量。为此，对可决系数的统计的 可靠性也应进行检验。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">```</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## (3)算法</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### 分类算法：</span></span><br><span class="line"></span><br><span class="line">● 线性算法 </span><br><span class="line"></span><br><span class="line">​		逻辑回归 ```LogisticRegression(multi_class=<span class="string">&#x27;multinomial&#x27;</span>, max_iter=<span class="number">3000</span>)```</span><br><span class="line"></span><br><span class="line">​		线性判别分析```LinearDiscriminantAnalysis```</span><br><span class="line"></span><br><span class="line">​	 ● 非线性算法 </span><br><span class="line"></span><br><span class="line">​		K近邻 ```KNeighborsClassifier```</span><br><span class="line"></span><br><span class="line">​		贝叶斯分类器 ```GaussianNB```</span><br><span class="line"></span><br><span class="line">​		分类与回归树 ```DecisionTreeClassifier```</span><br><span class="line"></span><br><span class="line">​		支持向量```SVC```</span><br><span class="line"></span><br><span class="line"><span class="comment">### 回归算法：</span></span><br><span class="line"></span><br><span class="line">​	● 线性回归``` LinearRegression```</span><br><span class="line"></span><br><span class="line">​	● 岭回归</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## (4)优化</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### ①装袋(Bagging)算法</span></span><br><span class="line"></span><br><span class="line">​	先将训练集分离成多个子集，然后通过各个子集训练多个模型。装袋算法是一种提高分类准确率的算法，通过给定组合投票的方式获得最优解。</span><br><span class="line"></span><br><span class="line">​	**<span class="number">1.</span> 装袋决策树(Bagged Decision Trees)** </span><br><span class="line"></span><br><span class="line">​		装袋决策树装袋算法在**数据具有很大的方差**时非常有效，最常见的例子就是决策树的装袋算法。</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line">num_folds = <span class="number">10</span></span><br><span class="line">seed = <span class="number">7</span></span><br><span class="line">kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">cart = DecisionTreeClassifier()</span><br><span class="line">num_tree = <span class="number">100</span></span><br><span class="line">model = BaggingClassifier(base_estimator=cart, n_estimators=num_tree, random_state=seed)</span><br><span class="line"></span><br><span class="line">result = cross_val_score(model, X, Y, cv=kfold)</span><br><span class="line"><span class="built_in">print</span>(result.mean())</span><br></pre></td></tr></table></figure>
<p>​    <strong>2. 随机森林(Random Forest)</strong></p>
<p>​    我的理解：每一棵决策树就是一个精通某一个领域的专家，这样在随机森林中就有了很多个 精通不同领域的专家，对于一个新的问题（新的输入数据)，可以从不同的角度去看待它，最 终由各个专家投票得到结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line">num_tree = <span class="number">100</span></span><br><span class="line">max_features = <span class="number">3</span></span><br><span class="line">model = RandomForestClassifier(n_estimators=num_tree, random_state=seed, max_features=max_features)</span><br></pre></td></tr></table></figure>
<p>​    <strong>3. 极端随机树(Extra Trees)</strong></p>
<p>​    随机森林应用的是Bagging模型，而极端随机树是使用所有的训练样本得到每棵决策 树，也就是每棵决策树应用的是相同的全部训练样本。</p>
<p>​    随机森林应用的是Bagging模型，而极端随机树是使用所有的训练样本得到每棵决策 树，也就是每棵决策树应用的是相同的全部训练样本。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> ExtraTreesClassifier</span><br><span class="line"></span><br><span class="line">num_tree = <span class="number">100</span></span><br><span class="line">max_features = <span class="number">7</span></span><br><span class="line">model = ExtraTreesClassifier(n_estimators=num_tree, random_state=seed, max_features=max_features)</span><br></pre></td></tr></table></figure>
<h3 id="②提升-Boosting-算法"><a href="#②提升-Boosting-算法" class="headerlink" title="②提升(Boosting)算法"></a>②提升(Boosting)算法</h3><p>​    提升算法是一种用来提高弱分类(分类不明显)算法准确度的方法，这种方法先构造一个预测函数系列， 然后以一定的方式将它们组合成一个预测函数。</p>
<p>​    <strong>1.AdaBoost</strong></p>
<p>​        一种迭代算法，其核心思想是针对同一个训练集训练不同的分类器 （弱分类器)，然后把这些弱分类器集合起来，构成一个更强的最终分类器（强分类器）.其算法本身是通过改变数据分布来实现的，它根据每次训练集中每个样本的分类是否正 确，以及上次的总体分类的准确率，来确定每个样本的权值。它将修改过权值的新数据集送给下层分类器进行训练，再将每次训练得到的分类器融合 起来，作为最后的决策分类器。使用AdaBoost分类器可以排除一些不必要的训练数据特征，并放在关键的训练数据上面。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line"></span><br><span class="line">num_tree = <span class="number">30</span></span><br><span class="line">model = AdaBoostClassifier(n_estimators=num_tree, random_state=seed)</span><br></pre></td></tr></table></figure>
<p>​    <strong>2. 随机梯度提升(Stochastic Gradient Boosting)</strong></p>
<p>​            要找到某个函数的最大值，最好的办法就是沿着该函数的梯度方向探寻。 梯度算子总是指向函数值增长最快的方向。由于梯度提升算法在每次更新数据集时都需要遍历整个数据集，计算复杂度较高，于是 有了一个改进算法一随机梯度提升算法，该算法一次只用一个样本点来更新回归系数， 极大地改善了算法的计算复杂度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line"></span><br><span class="line">num_tree = <span class="number">100</span></span><br><span class="line">model = GradientBoostingClassifier(n_estimators=num_tree, random_state=seed)</span><br></pre></td></tr></table></figure>
<h3 id="③投票-Voting-算法"><a href="#③投票-Voting-算法" class="headerlink" title="③投票(Voting)算法"></a>③投票(Voting)算法</h3><p>​        是一个非常简单的多个机器学习算法的集成算法。投票算法是通过创建两个或多个算法模型，利用投票算法将这些算法包装起来，计算各 个子模型的平均预测状况。在实际的应用中，可以对每个子模型的预测结果增加权重，以提高算法的准确度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> read_csv</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> VotingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line">cart = DecisionTreeClassifier()</span><br><span class="line">models = []</span><br><span class="line">model_logistic = LogisticRegression()</span><br><span class="line">models.append((<span class="string">&#x27;logistic&#x27;</span>, model_logistic))</span><br><span class="line">model_cart = DecisionTreeClassifier()</span><br><span class="line">models.append((<span class="string">&#x27;cart&#x27;</span>, model_cart))</span><br><span class="line">model_svc = SVC()</span><br><span class="line">models.append((<span class="string">&#x27;svm&#x27;</span>, model_svc))</span><br><span class="line">ensemble_model = VotingClassifier(estimators=models)</span><br><span class="line">result = cross_val_score(ensemble_model, X, Y , cv=kfold)</span><br><span class="line"><span class="built_in">print</span>(result.mean())</span><br></pre></td></tr></table></figure>
<h2 id="5-调参"><a href="#5-调参" class="headerlink" title="(5)调参"></a>(5)调参</h2><p>​        目的：提高稳定性，减小偏差和方差。</p>
<p>​        参数有两类：准确度&amp;防止过拟合。</p>
<h3 id="①网络搜索优化调参"><a href="#①网络搜索优化调参" class="headerlink" title="①网络搜索优化调参"></a>①网络搜索优化调参</h3><p>​        通过遍历已定义参数的列表，来评 估算法的参数，从而找到最优参数。但是，只在自己设置的超参数里选取。一般用于参数&lt;3。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> read_csv</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line">X = array[:, <span class="number">0</span>:<span class="number">8</span>]</span><br><span class="line">Y = array[:, <span class="number">8</span>]</span><br><span class="line">model = Ridge()</span><br><span class="line">param_grid = &#123;<span class="string">&#x27;alpha&#x27;</span> : [<span class="number">1</span>, <span class="number">0.1</span>, <span class="number">0.01</span>, <span class="number">0.001</span>, <span class="number">0</span>]&#125;	<span class="comment">#设置遍历的参数</span></span><br><span class="line">grid = GridSearchCV(estimator=model, param_grid=param_grid)</span><br><span class="line">grid.fit(X, Y)</span><br><span class="line"><span class="comment">#结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;最高得分： %.3f&#x27;</span>  % grid.best_score_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;最优参数：%s&#x27;</span> % grid.best_estimator_.alpha)</span><br></pre></td></tr></table></figure>
<h3 id="②随机搜索优化调参"><a href="#②随机搜索优化调参" class="headerlink" title="②随机搜索优化调参"></a>②随机搜索优化调参</h3><p>​        通过固定次数的迭代，采用随机采样分布的方式搜索合适的参数。</p>
<p>​        与网格搜索优化参数相比，随机搜索优化参数提供了一种更高效的解决方法（特别是在 参数数量多的情况下），随机搜索优化参数为每个参数定义了一个分布函数，并在该空 间中采样。</p>
<p>​        一般用于参数&gt;3。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RandomizedSearchCV</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> uniform</span><br><span class="line"></span><br><span class="line">model = Ridge()</span><br><span class="line">param_grid = &#123;<span class="string">&#x27;alpha&#x27;</span>: uniform()&#125;	<span class="comment">#设置遍历的参数</span></span><br><span class="line">grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=<span class="number">100</span>, random_state=<span class="number">7</span>)</span><br><span class="line">grid.fit(X, Y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;最高得分：%.3f&#x27;</span> % grid.best_score_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;最优参数：%s&#x27;</span> % grid.best_estimator_.alpha)</span><br></pre></td></tr></table></figure>
<h2 id="6-实战"><a href="#6-实战" class="headerlink" title="(6)实战"></a>(6)实战</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入类库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> arange</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> read_csv</span><br><span class="line"><span class="keyword">from</span> pandas.plotting <span class="keyword">import</span> scatter_matrix</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> ElasticNet</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVR</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> ExtraTreesRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"></span><br><span class="line"><span class="comment">#导入数据</span></span><br><span class="line">filename = <span class="string">&#x27;housing.csv&#x27;</span></span><br><span class="line">names = [<span class="string">&#x27;CRIM&#x27;</span>, <span class="string">&#x27;ZN&#x27;</span>, <span class="string">&#x27;INDUS&#x27;</span>, <span class="string">&#x27;CHAS&#x27;</span>, <span class="string">&#x27;NOX&#x27;</span>, <span class="string">&#x27;RM&#x27;</span>, <span class="string">&#x27;AGE&#x27;</span>,</span><br><span class="line">         <span class="string">&#x27;DIS&#x27;</span>,<span class="string">&#x27;RAD&#x27;</span>, <span class="string">&#x27;TAX&#x27;</span>, <span class="string">&#x27;PRTATIO&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;LSTAT&#x27;</span>, <span class="string">&#x27;MEDV&#x27;</span>]</span><br><span class="line">data = read_csv(filename, names=names, delim_whitespace=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(data.shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(data.dtypes)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(data.head(<span class="number">30</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(data.describe())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(data.corr(method=<span class="string">&#x27;pearson&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据的可视化</span></span><br><span class="line">data.hist(sharex=<span class="literal">False</span>, sharey=<span class="literal">False</span>, xlabelsize=<span class="number">1</span>, ylabelsize=<span class="number">1</span>)</span><br><span class="line">pyplot.show()</span><br><span class="line">data.plot(kind=<span class="string">&#x27;density&#x27;</span>, subplots=<span class="literal">True</span>, layout=(<span class="number">4</span>, <span class="number">4</span>), sharex=<span class="literal">False</span>, fontsize=<span class="number">1</span>)</span><br><span class="line">pyplot.show()</span><br><span class="line"></span><br><span class="line">data.plot(kind=<span class="string">&#x27;box&#x27;</span>, subplots=<span class="literal">True</span>, layout=(<span class="number">4</span>, <span class="number">4</span>),</span><br><span class="line">          sharex=<span class="literal">False</span>, sharey=<span class="literal">False</span>, fontsize=<span class="number">8</span>)</span><br><span class="line">pyplot.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#散点图</span></span><br><span class="line">scatter_matrix(data)</span><br><span class="line">pyplot.show()</span><br><span class="line"><span class="comment"># 矩阵图</span></span><br><span class="line">fig = pyplot.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">cax = ax.matshow(data.corr(), vmin=-<span class="number">1</span>, vmax=<span class="number">1</span>, interpolation=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">fig.colorbar(cax)</span><br><span class="line">ticks = np.arange(<span class="number">0</span>, <span class="number">14</span>, <span class="number">1</span>)</span><br><span class="line">ax.set_xticks(ticks)</span><br><span class="line">ax.set_yticks(ticks)</span><br><span class="line">ax.set_xticklabels(names)</span><br><span class="line">ax.set_yticklabels(names)</span><br><span class="line">pyplot.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#特征选择， 标准化数据， 正太化数据</span></span><br><span class="line"><span class="comment">#数据分离</span></span><br><span class="line">array = data.values</span><br><span class="line">X =  array[:, <span class="number">0</span>:<span class="number">13</span>]</span><br><span class="line">Y = array[:, <span class="number">13</span>]</span><br><span class="line">validation_size = <span class="number">0.2</span></span><br><span class="line">seed = <span class="number">7</span></span><br><span class="line">num_folds = <span class="number">10</span></span><br><span class="line">scoring = <span class="string">&#x27;neg_mean_squared_error&#x27;</span></span><br><span class="line">X_train, X_validation, Y_train, Y_validation =train_test_split(X, Y,</span><br><span class="line">                                                               test_size=validation_size,</span><br><span class="line">                                                               random_state=seed)</span><br><span class="line"></span><br><span class="line"><span class="comment">#评估算法</span></span><br><span class="line">num_folds = <span class="number">10</span></span><br><span class="line">seed = <span class="number">7</span></span><br><span class="line">scoring = <span class="string">&#x27;neg_mean_squared_error&#x27;</span></span><br><span class="line">models = &#123;&#125;</span><br><span class="line">models[<span class="string">&#x27;LR&#x27;</span>] = LinearRegression()</span><br><span class="line">models[<span class="string">&#x27;LASSO&#x27;</span>] = Lasso()</span><br><span class="line">models[<span class="string">&#x27;EN&#x27;</span>] = ElasticNet()</span><br><span class="line">models[<span class="string">&#x27;KNN&#x27;</span>] = KNeighborsRegressor()</span><br><span class="line">models[<span class="string">&#x27;CART&#x27;</span>] = DecisionTreeRegressor()</span><br><span class="line">models[<span class="string">&#x27;SVM&#x27;</span>] = SVR()</span><br><span class="line">results = []</span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> models:</span><br><span class="line">    kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=<span class="literal">True</span>)</span><br><span class="line">    cv_result = cross_val_score(models[key], X_train, Y_train, cv=kfold, scoring=scoring)</span><br><span class="line">    results.append(cv_result)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;%s: %f(%f)&#x27;</span> %(key, cv_result.mean(), cv_result.std()))</span><br><span class="line"></span><br><span class="line">fig = pyplot.figure()</span><br><span class="line">fig.suptitle(<span class="string">&#x27;Algorithm Comparision&#x27;</span>)</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">pyplot.boxplot(results)</span><br><span class="line">ax.set_xticklabels(models.keys())</span><br><span class="line">pyplot.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估算法 - 正态化数据</span></span><br><span class="line">pipelines = &#123;&#125;</span><br><span class="line">pipelines[<span class="string">&#x27;ScalerLR&#x27;</span>] = Pipeline([(<span class="string">&#x27;Scaler&#x27;</span>, StandardScaler()), (<span class="string">&#x27;LR&#x27;</span>, LinearRegression())])</span><br><span class="line">pipelines[<span class="string">&#x27;ScalerLASSO&#x27;</span>] = Pipeline([(<span class="string">&#x27;Scaler&#x27;</span>, StandardScaler()), (<span class="string">&#x27;LASSO&#x27;</span>, Lasso())])</span><br><span class="line">pipelines[<span class="string">&#x27;ScalerEN&#x27;</span>] = Pipeline([(<span class="string">&#x27;Scaler&#x27;</span>, StandardScaler()), (<span class="string">&#x27;EN&#x27;</span>, ElasticNet())])</span><br><span class="line">pipelines[<span class="string">&#x27;ScalerKNN&#x27;</span>] = Pipeline([(<span class="string">&#x27;Scaler&#x27;</span>, StandardScaler()), (<span class="string">&#x27;KNN&#x27;</span>, KNeighborsRegressor())])</span><br><span class="line">pipelines[<span class="string">&#x27;ScalerCART&#x27;</span>] = Pipeline([(<span class="string">&#x27;Scaler&#x27;</span>, StandardScaler()), (<span class="string">&#x27;CART&#x27;</span>, DecisionTreeRegressor())])</span><br><span class="line">pipelines[<span class="string">&#x27;ScalerSVM&#x27;</span>] = Pipeline([(<span class="string">&#x27;Scaler&#x27;</span>, StandardScaler()), (<span class="string">&#x27;SVM&#x27;</span>, SVR())])</span><br><span class="line">results = []</span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> pipelines:</span><br><span class="line">    kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=<span class="literal">True</span>)</span><br><span class="line">    cv_result = cross_val_score(pipelines[key], X_train, Y_train, cv=kfold, scoring=scoring)</span><br><span class="line">    results.append(cv_result)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;%s: %f (%f)&#x27;</span> % (key, cv_result.mean(), cv_result.std()))</span><br><span class="line"><span class="comment">#评估算法 - 箱线图</span></span><br><span class="line">fig = pyplot.figure()</span><br><span class="line">fig.suptitle(<span class="string">&#x27;Algorithm Comparison&#x27;</span>)</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">pyplot.boxplot(results)</span><br><span class="line">ax.set_xticklabels(pipelines.keys())</span><br><span class="line">pyplot.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#调参</span></span><br><span class="line">scaler = StandardScaler().fit(X_train)</span><br><span class="line">rescalerX = scaler.transform(X_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># rescaler = StandardScaler().fit_transform(X_train)</span></span><br><span class="line">para_grid = &#123;<span class="string">&#x27;n_neighbores&#x27;</span>: [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">11</span>, <span class="number">13</span>, <span class="number">15</span>, <span class="number">17</span>, <span class="number">19</span>, <span class="number">21</span>]&#125;</span><br><span class="line"></span><br><span class="line">model = KNeighborsRegressor()</span><br><span class="line">kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=<span class="literal">True</span>)</span><br><span class="line">grid = GridSearchCV(estimator=model, param_grid=para_grid, scoring=scoring, cv=kfold)</span><br><span class="line">grid_result = grid.fit(X=rescalerX, y=Y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;最优： %s 使用%s&#x27;</span> %(grid_result.best_score_, grid_result.best_params_))</span><br><span class="line">cv_results = <span class="built_in">zip</span>(grid_result.cv_results_[<span class="string">&#x27;mean_test_score&#x27;</span>],</span><br><span class="line">                 grid_result.cv_results_[<span class="string">&#x27;std_test_score&#x27;</span>],</span><br><span class="line">                 grid_result.cv_results_[<span class="string">&#x27;params&#x27;</span>])</span><br><span class="line"><span class="keyword">for</span> mean, std, param <span class="keyword">in</span> cv_results:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;%f(%f) with %r&#x27;</span> %(mean, std, param))</span><br><span class="line"></span><br><span class="line">scaler = StandardScaler().fit(X_train)</span><br><span class="line">rescaledX = scaler.transform(X_train)</span><br><span class="line">param_grid = &#123;<span class="string">&#x27;n_neighbors&#x27;</span>: [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">11</span>, <span class="number">13</span>, <span class="number">15</span>, <span class="number">17</span>, <span class="number">19</span>, <span class="number">21</span>]&#125;</span><br><span class="line">model = KNeighborsRegressor()</span><br><span class="line">kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=<span class="literal">True</span>)</span><br><span class="line">grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)</span><br><span class="line">grid_result = grid.fit(X=rescaledX, y=Y_train)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;最优：%s 使用%s&#x27;</span> % (grid_result.best_score_, grid_result.best_params_))</span><br><span class="line">cv_results = <span class="built_in">zip</span>(grid_result.cv_results_[<span class="string">&#x27;mean_test_score&#x27;</span>],</span><br><span class="line">                 grid_result.cv_results_[<span class="string">&#x27;std_test_score&#x27;</span>],</span><br><span class="line">                 grid_result.cv_results_[<span class="string">&#x27;params&#x27;</span>])</span><br><span class="line"><span class="keyword">for</span> mean, std, param <span class="keyword">in</span> cv_results:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;%f (%f) with %r&#x27;</span> % (mean, std, param))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#集成算法</span></span><br><span class="line">ensembles = &#123;&#125;</span><br><span class="line">ensembles[<span class="string">&#x27;ScaledAB&#x27;</span>] = Pipeline([(<span class="string">&#x27;Scaler&#x27;</span>, StandardScaler()), (<span class="string">&#x27;AB&#x27;</span>, AdaBoostRegressor())])</span><br><span class="line">ensembles[<span class="string">&#x27;ScaledAB-KNN&#x27;</span>] = Pipeline([(<span class="string">&#x27;Scaler&#x27;</span>, StandardScaler()),</span><br><span class="line">                                      (<span class="string">&#x27;ABKNN&#x27;</span>, AdaBoostRegressor(base_estimator=KNeighborsRegressor(n_neighbors=<span class="number">1</span>)))])</span><br><span class="line">ensembles[<span class="string">&#x27;ScaledAB-LR&#x27;</span>] = Pipeline([(<span class="string">&#x27;Scaler&#x27;</span>, StandardScaler()),</span><br><span class="line">                                     (<span class="string">&#x27;ABLR&#x27;</span>, AdaBoostRegressor(LinearRegression()))])</span><br><span class="line">ensembles[<span class="string">&#x27;ScaledRFR&#x27;</span>] = Pipeline([(<span class="string">&#x27;Scaler&#x27;</span>, StandardScaler()),</span><br><span class="line">                                   (<span class="string">&#x27;RFR&#x27;</span>, RandomForestRegressor())])</span><br><span class="line">ensembles[<span class="string">&#x27;ScaledETR&#x27;</span>] = Pipeline([(<span class="string">&#x27;Scaler&#x27;</span>, StandardScaler()),</span><br><span class="line">                                   (<span class="string">&#x27;ETR&#x27;</span>, ExtraTreesRegressor())])</span><br><span class="line">ensembles[<span class="string">&#x27;ScaledGBR&#x27;</span>] = Pipeline([(<span class="string">&#x27;Scaler&#x27;</span>, StandardScaler()),</span><br><span class="line">                                  (<span class="string">&#x27;RBR&#x27;</span>, GradientBoostingRegressor())])</span><br><span class="line">results = []</span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> ensembles:</span><br><span class="line">    kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=<span class="literal">True</span>)</span><br><span class="line">    cv_result = cross_val_score(ensembles[key], X_train, Y_train, cv=kfold, scoring=scoring)</span><br><span class="line">    results.append(cv_result)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;%s: %f (%f)&#x27;</span> % (key, cv_result.mean(), cv_result.std()))</span><br><span class="line"></span><br><span class="line"><span class="comment">#集成算法——箱线图</span></span><br><span class="line">fig = pyplot.figure()</span><br><span class="line">fig.suptitle(<span class="string">&#x27;Comparison&#x27;</span>)</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">pyplot.boxplot(results)</span><br><span class="line">ax.set_xticklabels(ensembles.keys())</span><br><span class="line">pyplot.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#集成算法调参 n_estimators</span></span><br><span class="line">caler = StandardScaler().fit(X_train)</span><br><span class="line">rescaledX = scaler.transform(X_train)</span><br><span class="line">param_grid = &#123;<span class="string">&#x27;n_estimators&#x27;</span>:[<span class="number">10</span>, <span class="number">50</span>, <span class="number">100</span>, <span class="number">200</span>, <span class="number">300</span>, <span class="number">400</span>, <span class="number">500</span>, <span class="number">600</span>,</span><br><span class="line">                              <span class="number">700</span>, <span class="number">800</span>, <span class="number">900</span>, <span class="number">950</span>]&#125;</span><br><span class="line">model = GradientBoostingRegressor()</span><br><span class="line">kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=<span class="literal">True</span>)</span><br><span class="line">grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)</span><br><span class="line">grid_result = grid.fit(X=rescaledX, y=Y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;最优： %s 使用 %s&#x27;</span> % (grid_result.best_score_, grid_result.best_params_))</span><br><span class="line"></span><br><span class="line"><span class="comment">#集成算法——ET调参</span></span><br><span class="line">scaler = StandardScaler().fit(X_train)</span><br><span class="line">rescaledX = scaler.transform(X_train)</span><br><span class="line">param_grid = &#123;<span class="string">&#x27;n_estimators&#x27;</span>:[<span class="number">10</span>, <span class="number">50</span>, <span class="number">100</span>, <span class="number">200</span>, <span class="number">300</span>, <span class="number">400</span>, <span class="number">500</span>, <span class="number">600</span>,</span><br><span class="line">                              <span class="number">700</span>, <span class="number">800</span>, <span class="number">900</span>, <span class="number">950</span>]&#125;</span><br><span class="line">model = ExtraTreesRegressor()</span><br><span class="line">kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=<span class="literal">True</span>)</span><br><span class="line">grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)</span><br><span class="line">grid_result = grid.fit(X=rescaledX, y=Y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;最优： %s 使用 %s&#x27;</span> % (grid_result.best_score_, grid_result.best_params_))</span><br><span class="line"></span><br><span class="line"><span class="comment">#模型的确立</span></span><br><span class="line">caler = StandardScaler().fit(X_train)</span><br><span class="line">rescaledX = scaler.transform(X_train)</span><br><span class="line">gbr = ExtraTreesRegressor(n_estimators=<span class="number">600</span>)</span><br><span class="line">gbr.fit(X=rescaledX, y=Y_train)</span><br><span class="line"><span class="comment">#评估算法</span></span><br><span class="line">rescaledX_validation = scaler.transform(X_validation)</span><br><span class="line">predictions = gbr.predict(rescaledX_validation)</span><br><span class="line"><span class="built_in">print</span>(mean_squared_error(Y_validation, predictions))</span><br><span class="line">x_new =[<span class="number">1</span>, <span class="number">2</span>, ]</span><br></pre></td></tr></table></figure>
<p>参考资料：</p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Hh4y1w7fZ/?spm_id_from=333.999.0.0&amp;vd_source=15aaf2f3032abe28c98a76535d8c050b">Python数学建模与分析：基础入门、数据处理、算法编程、高级绘图、建模实战！bilibili</a></p>
<h1 id="五、绘图"><a href="#五、绘图" class="headerlink" title="五、绘图"></a>五、绘图</h1><h2 id="1-箱型图"><a href="#1-箱型图" class="headerlink" title="(1)箱型图"></a>(1)箱型图</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">data = pandas.read_excel(<span class="string">&#x27;time.xlsx&#x27;</span> , index_col=<span class="string">&#x27;日期&#x27;</span>)</span><br><span class="line"></span><br><span class="line">data.boxplot()	<span class="comment">#简单方式</span></span><br><span class="line"></span><br><span class="line">data.boxplot(sym=<span class="string">&#x27;r*&#x27;</span>,vert=<span class="literal">False</span>,patch_artist=<span class="literal">True</span>,meanline=<span class="literal">False</span>,showmeans=<span class="literal">True</span>)	<span class="comment">#横着</span></span><br><span class="line"></span><br><span class="line">plt.boxplot(x = data, <span class="comment"># 指定绘制箱线图的数据</span></span><br><span class="line">         whis = <span class="number">1.5</span>, <span class="comment"># 指定1.5倍的四分位差</span></span><br><span class="line">         widths = <span class="number">0.7</span>, <span class="comment"># 指定箱线图的宽度为0.8</span></span><br><span class="line">         patch_artist = <span class="literal">True</span>, <span class="comment"># 指定需要填充箱体颜色</span></span><br><span class="line">         showmeans = <span class="literal">True</span>, <span class="comment"># 指定需要显示均值</span></span><br><span class="line">         boxprops = &#123;<span class="string">&#x27;facecolor&#x27;</span>:<span class="string">&#x27;steelblue&#x27;</span>&#125;, <span class="comment"># 指定箱体的填充色为铁蓝色</span></span><br><span class="line">        <span class="comment"># 指定异常点的填充色、边框色和大小</span></span><br><span class="line">         flierprops = &#123;<span class="string">&#x27;markerfacecolor&#x27;</span>:<span class="string">&#x27;red&#x27;</span>, <span class="string">&#x27;markeredgecolor&#x27;</span>:<span class="string">&#x27;red&#x27;</span>, <span class="string">&#x27;markersize&#x27;</span>:<span class="number">4</span>&#125;, </span><br><span class="line">         <span class="comment"># 指定均值点的标记符号（菱形）、填充色和大小</span></span><br><span class="line">        meanprops = &#123;<span class="string">&#x27;marker&#x27;</span>:<span class="string">&#x27;D&#x27;</span>,<span class="string">&#x27;markerfacecolor&#x27;</span>:<span class="string">&#x27;black&#x27;</span>, <span class="string">&#x27;markersize&#x27;</span>:<span class="number">4</span>&#125;, </span><br><span class="line">         medianprops = &#123;<span class="string">&#x27;linestyle&#x27;</span>:<span class="string">&#x27;--&#x27;</span>,<span class="string">&#x27;color&#x27;</span>:<span class="string">&#x27;orange&#x27;</span>&#125;, <span class="comment"># 指定中位数的标记符号（虚线）和颜色</span></span><br><span class="line">         labels = [<span class="string">&#x27;&#x27;</span>] <span class="comment"># 去除箱线图的x轴刻度值</span></span><br><span class="line">         )</span><br><span class="line"><span class="comment"># 显示图形</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#删除异常值</span></span><br><span class="line"><span class="comment"># 计算下四分位数和上四分位</span></span><br><span class="line">Q1 = data[<span class="string">&#x27;销量&#x27;</span>].quantile(q = <span class="number">0.25</span>)</span><br><span class="line">Q3 = data[<span class="string">&#x27;销量&#x27;</span>].quantile(q = <span class="number">0.75</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 基于1.5倍的四分位差计算上下须对应的值</span></span><br><span class="line">low_whisker = Q1 - <span class="number">1.5</span>*(Q3 - Q1)</span><br><span class="line">up_whisker = Q3 + <span class="number">1.5</span>*(Q3 - Q1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 寻找异常点</span></span><br><span class="line">data[<span class="string">&#x27;销量&#x27;</span>][(data[<span class="string">&#x27;销量&#x27;</span>] &gt; up_whisker) | (data[<span class="string">&#x27;销量&#x27;</span>] &lt; low_whisker)]</span><br></pre></td></tr></table></figure>
<h2 id="2-正太分布图"><a href="#2-正太分布图" class="headerlink" title="(2)正太分布图"></a>(2)正太分布图</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读入外部数据</span></span><br><span class="line">pay_ratio = pd.read_excel(<span class="string">r&#x27;time.xlsx&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">35</span>, <span class="number">6</span>))</span><br><span class="line"><span class="comment"># 绘制单条折线图，并在折线图的基础上添加点图</span></span><br><span class="line">plt.plot(pay_ratio[<span class="string">&#x27;日期&#x27;</span>], pay_ratio[<span class="string">&#x27;销量&#x27;</span>],</span><br><span class="line">         linestyle=<span class="string">&#x27;-&#x27;</span>, linewidth=<span class="number">2</span>, color=<span class="string">&#x27;steelblue&#x27;</span>,</span><br><span class="line">         marker=<span class="string">&#x27;o&#x27;</span>, markersize=<span class="number">4</span>, markeredgecolor=<span class="string">&#x27;black&#x27;</span>, markerfacecolor=<span class="string">&#x27;black&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加上下界的水平参考线</span></span><br><span class="line">plt.axhline(y=pay_ratio[<span class="string">&#x27;销量&#x27;</span>].mean() - <span class="number">2</span> * pay_ratio[<span class="string">&#x27;销量&#x27;</span>].std(), linestyle=<span class="string">&#x27;--&#x27;</span>, color=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">plt.axhline(y=pay_ratio[<span class="string">&#x27;销量&#x27;</span>].mean() + <span class="number">2</span> * pay_ratio[<span class="string">&#x27;销量&#x27;</span>].std(), linestyle=<span class="string">&#x27;--&#x27;</span>, color=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置日期的显示格式</span></span><br><span class="line">date_format = mpl.dates.DateFormatter(<span class="string">&quot;%m-%d&quot;</span>)</span><br><span class="line">ax = plt.gca()</span><br><span class="line">ax.xaxis.set_major_formatter(date_format)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置x轴每个刻度的间隔天数</span></span><br><span class="line">xlocator = mpl.ticker.MultipleLocator(<span class="number">7</span>)</span><br><span class="line">ax.xaxis.set_major_locator(xlocator)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了避免x轴刻度标签的紧凑，将刻度标签旋转45度</span></span><br><span class="line">plt.xticks(rotation=<span class="number">45</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示图形</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>筛异常点</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算判断异常点和极端异常点的临界值</span></span><br><span class="line">outlier_ll = pay_ratio[<span class="string">&#x27;销量&#x27;</span>].mean() - <span class="number">2</span> * pay_ratio[<span class="string">&#x27;销量&#x27;</span>].std()</span><br><span class="line">outlier_ul = pay_ratio[<span class="string">&#x27;销量&#x27;</span>].mean() + <span class="number">2</span> * pay_ratio[<span class="string">&#x27;销量&#x27;</span>].std()</span><br><span class="line"></span><br><span class="line">extreme_outlier_ll = pay_ratio[<span class="string">&#x27;销量&#x27;</span>].mean() - <span class="number">3</span> * pay_ratio[<span class="string">&#x27;销量&#x27;</span>].std()</span><br><span class="line">extreme_outlier_ul = pay_ratio[<span class="string">&#x27;销量&#x27;</span>].mean() + <span class="number">3</span> * pay_ratio[<span class="string">&#x27;销量&#x27;</span>].std()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 寻找异常点</span></span><br><span class="line">pay_ratio.loc[(pay_ratio[<span class="string">&#x27;销量&#x27;</span>] &gt; outlier_ul) | (pay_ratio[<span class="string">&#x27;销量&#x27;</span>] &lt; outlier_ll), [<span class="string">&#x27;日期&#x27;</span>, <span class="string">&#x27;销量&#x27;</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 寻找极端异常点</span></span><br><span class="line">pay_ratio.loc[(pay_ratio[<span class="string">&#x27;销量&#x27;</span>] &gt; extreme_outlier_ul) | (pay_ratio[<span class="string">&#x27;销量&#x27;</span>] &lt; extreme_outlier_ll), [<span class="string">&#x27;日期&#x27;</span>, <span class="string">&#x27;销量&#x27;</span>]]</span><br></pre></td></tr></table></figure>
<h1 id="六、聚类分析-无监督分析"><a href="#六、聚类分析-无监督分析" class="headerlink" title="六、聚类分析(无监督分析)"></a>六、聚类分析(无监督分析)</h1><h2 id="1-k-means"><a href="#1-k-means" class="headerlink" title="(1)k-means"></a>(1)k-means</h2><p>​        如果K值未知，可采用肘部法选择K值（假设最大分类数为9类，分别计算分类结果为1-9类的平均离差，离差的提升变化下降最抖时的值为最优聚类数K）</p>
<p>​    //纯数组X作为输入</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">1.4889993</span>  <span class="number">4.18741329</span>]</span><br><span class="line"> [<span class="number">3.95221785</span> <span class="number">3.76674812</span>]</span><br><span class="line"> [<span class="number">4.09826192</span> <span class="number">3.95063903</span>]</span><br><span class="line"> [<span class="number">3.65208848</span> <span class="number">4.44383585</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">from</span> scipy.spatial.distance <span class="keyword">import</span> cdist</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"> </span><br><span class="line">data=pd.read_excel(<span class="string">&#x27;data.xlsx&#x27;</span>,header=<span class="number">0</span>).iloc[:<span class="number">501</span>,<span class="number">3</span>:<span class="number">5</span>]</span><br><span class="line">X=np.array(data)</span><br><span class="line"> </span><br><span class="line">K=<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">10</span>)</span><br><span class="line">meanDispersions=[]</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> K:</span><br><span class="line">    kemans=KMeans(n_clusters=k)</span><br><span class="line">    kemans.fit(X)</span><br><span class="line">    meanDispersions.append(<span class="built_in">sum</span>(np.<span class="built_in">min</span>(cdist(X,kemans.cluster_centers_,<span class="string">&#x27;euclidean&#x27;</span>),axis=<span class="number">1</span>))/X.shape[<span class="number">0</span>])</span><br><span class="line"> </span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.family&#x27;</span>] = [<span class="string">&#x27;sans-serif&#x27;</span>]</span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>]</span><br><span class="line">plt.plot(K,meanDispersions,<span class="string">&#x27;bx-&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;平均离差&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;用肘部方法选择K值&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 具体聚类过程</span></span><br><span class="line">kmeans=KMeans(n_clusters=<span class="number">3</span>)</span><br><span class="line">result=kmeans.fit_predict(X)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line">x=[i[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> X]</span><br><span class="line">y=[i[<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> X]</span><br><span class="line">plt.scatter(x,y,c=result,marker=<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;title&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;聚类标签&#x27;</span>, kmeans.labels_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;聚类中心&#x27;</span>,kmeans.cluster_centers_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;聚类准则总和&#x27;</span>,kmeans.inertia_)</span><br></pre></td></tr></table></figure>
<p>​        聚类有效性评价: Rand指数、轮廓系数（Silhouette Coefficient）、Calinski-Harabaz .</p>
<p>​        轮廓系数∈[-1,1]，越大表示簇间相似度高而不同簇相似度低，即聚类效果越好。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> silhouette_samples </span><br><span class="line">  </span><br><span class="line">y=[]</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>,<span class="number">23</span>):</span><br><span class="line">    kmeans=KMeans(n_clusters=n)</span><br><span class="line">    kmeans.fit(data)</span><br><span class="line">    label=kmeans.labels_</span><br><span class="line">    lkxs=silhouette_samples(data,label,metric=<span class="string">&#x27;euclidean&#x27;</span>)</span><br><span class="line">    means=np.mean(lkxs)</span><br><span class="line">    y.append(means)</span><br><span class="line"><span class="built_in">print</span>(y)</span><br></pre></td></tr></table></figure>
<p>参考：</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/TSzero/article/details/116265151">用python实现聚类分析<em>聚类分析python代码</em>米法·的博客-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/lk_mm_love/article/details/127040710">十种常用聚类算法（python完整代码演示）<em>python 聚类</em>小刘研CV的博客-CSDN博客</a></p>
<h1 id="七、评价模型"><a href="#七、评价模型" class="headerlink" title="七、评价模型"></a>七、评价模型</h1><h2 id="1-Topsis"><a href="#1-Topsis" class="headerlink" title="(1)Topsis"></a>(1)Topsis</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> xlrd</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read</span>(<span class="params">file</span>):</span><br><span class="line">    wb = xlrd.open_workbook(filename=file)<span class="comment">#打开文件</span></span><br><span class="line">    sheet = wb.sheet_by_index(<span class="number">0</span>)<span class="comment">#通过索引获取表格</span></span><br><span class="line">    rows = sheet.nrows <span class="comment"># 获取行数</span></span><br><span class="line">    all_content = []        <span class="comment">#存放读取的数据</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>):       <span class="comment">#取第1~第4列对的数据</span></span><br><span class="line">        temp = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,rows) :</span><br><span class="line">            cell = sheet.cell_value(i, j)   <span class="comment">#获取数据 </span></span><br><span class="line">            temp.append(cell)           </span><br><span class="line">        all_content.append(temp)    <span class="comment">#按列添加到结果集中</span></span><br><span class="line">        temp = []</span><br><span class="line">    <span class="keyword">return</span> np.array(all_content)</span><br><span class="line"></span><br><span class="line"><span class="comment">#极小型指标 -&gt; 极大型指标</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dataDirection_1</span>(<span class="params">datas</span>):         </span><br><span class="line">		<span class="keyword">return</span> np.<span class="built_in">max</span>(datas)-datas     <span class="comment">#套公式</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#中间型指标 -&gt; 极大型指标</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dataDirection_2</span>(<span class="params">datas, x_best</span>):</span><br><span class="line">    temp_datas = datas - x_best</span><br><span class="line">    M = np.<span class="built_in">max</span>(<span class="built_in">abs</span>(temp_datas))</span><br><span class="line">    answer_datas = <span class="number">1</span> - <span class="built_in">abs</span>(datas - x_best) / M     <span class="comment">#套公式</span></span><br><span class="line">    <span class="keyword">return</span> answer_datas</span><br><span class="line">    </span><br><span class="line"><span class="comment">#区间型指标 -&gt; 极大型指标</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dataDirection_3</span>(<span class="params">datas, x_min, x_max</span>):</span><br><span class="line">    M = <span class="built_in">max</span>(x_min - np.<span class="built_in">min</span>(datas), np.<span class="built_in">max</span>(datas) - x_max)</span><br><span class="line">    answer_list = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> datas:</span><br><span class="line">        <span class="keyword">if</span>(i &lt; x_min):</span><br><span class="line">            answer_list.append(<span class="number">1</span> - (x_min-i) /M)      <span class="comment">#套公式</span></span><br><span class="line">        <span class="keyword">elif</span>( x_min &lt;= i &lt;= x_max):</span><br><span class="line">            answer_list.append(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            answer_list.append(<span class="number">1</span> - (i - x_max)/M)</span><br><span class="line">    <span class="keyword">return</span> np.array(answer_list)   </span><br><span class="line"> </span><br><span class="line"><span class="comment">#正向化矩阵标准化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">temp2</span>(<span class="params">datas</span>):</span><br><span class="line">    K = np.power(np.<span class="built_in">sum</span>(<span class="built_in">pow</span>(datas,<span class="number">2</span>),axis =<span class="number">1</span>),<span class="number">0.5</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,K.size):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,datas[i].size):</span><br><span class="line">            datas[i,j] = datas[i,j] / K[i]      <span class="comment">#套用矩阵标准化的公式</span></span><br><span class="line">    <span class="keyword">return</span> datas</span><br><span class="line"></span><br><span class="line"><span class="comment">#计算得分并归一化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">temp3</span>(<span class="params">answer2</span>):</span><br><span class="line">    list_max = np.array([np.<span class="built_in">max</span>(answer2[<span class="number">0</span>,:]),np.<span class="built_in">max</span>(answer2[<span class="number">1</span>,:]),np.<span class="built_in">max</span>(answer2[<span class="number">2</span>,:]),np.<span class="built_in">max</span>(answer2[<span class="number">3</span>,:])])  <span class="comment">#获取每一列的最大值</span></span><br><span class="line">    list_min = np.array([np.<span class="built_in">min</span>(answer2[<span class="number">0</span>,:]),np.<span class="built_in">min</span>(answer2[<span class="number">1</span>,:]),np.<span class="built_in">min</span>(answer2[<span class="number">2</span>,:]),np.<span class="built_in">min</span>(answer2[<span class="number">3</span>,:])])  <span class="comment">#获取每一列的最小值</span></span><br><span class="line">    max_list = []       <span class="comment">#存放第i个评价对象与最大值的距离</span></span><br><span class="line">    min_list = []       <span class="comment">#存放第i个评价对象与最小值的距离</span></span><br><span class="line">    answer_list=[]      <span class="comment">#存放评价对象的未归一化得分</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,np.size(answer2,axis = <span class="number">1</span>)):        <span class="comment">#遍历每一列数据</span></span><br><span class="line">        max_sum = <span class="number">0</span></span><br><span class="line">        min_sum = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> q <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="number">4</span>):                                <span class="comment">#有四个指标</span></span><br><span class="line">            max_sum += np.power(answer2[q,k]-list_max[q],<span class="number">2</span>)     <span class="comment">#按每一列计算Di+</span></span><br><span class="line">            min_sum += np.power(answer2[q,k]-list_min[q],<span class="number">2</span>)     <span class="comment">#按每一列计算Di-</span></span><br><span class="line">        max_list.append(<span class="built_in">pow</span>(max_sum,<span class="number">0.5</span>))</span><br><span class="line">        min_list.append(<span class="built_in">pow</span>(min_sum,<span class="number">0.5</span>))</span><br><span class="line">        answer_list.append(min_list[k]/ (min_list[k] + max_list[k]))    <span class="comment">#套用计算得分的公式 Si = (Di-) / ((Di+) +(Di-))</span></span><br><span class="line">        max_sum = <span class="number">0</span></span><br><span class="line">        min_sum = <span class="number">0</span></span><br><span class="line">    answer = np.array(answer_list)      <span class="comment">#得分归一化</span></span><br><span class="line">    <span class="keyword">return</span> (answer / np.<span class="built_in">sum</span>(answer))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    file = <span class="string">&#x27;C:\\Users\\lenovo\Desktop\\数学建模\\TOPSIS法\\第2讲.TOPSIS法（优劣解距离法）7.17\\代码和例题数据\\20条河流的水质情况数据.xlsx&#x27;</span></span><br><span class="line">    answer1 = read(file)        <span class="comment">#读取文件</span></span><br><span class="line">    answer2 = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">4</span>):       <span class="comment">#按照不同的列，根据不同的指标转换为极大型指标，因为只有四列</span></span><br><span class="line">        answer = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span>(i == <span class="number">0</span>):             <span class="comment">#本来就是极大型指标，不用转换</span></span><br><span class="line">            answer = answer1[<span class="number">0</span>]             </span><br><span class="line">        <span class="keyword">elif</span>(i == <span class="number">1</span>):                   <span class="comment">#中间型指标</span></span><br><span class="line">            answer = dataDirection_2(answer1[<span class="number">1</span>],<span class="number">7</span>)</span><br><span class="line">        <span class="keyword">elif</span>(i==<span class="number">2</span>):                     <span class="comment">#极小型指标</span></span><br><span class="line">            answer = dataDirection_1(answer1[<span class="number">2</span>])</span><br><span class="line">        <span class="keyword">else</span>:                           <span class="comment">#范围型指标</span></span><br><span class="line">            answer = dataDirection_3(answer1[<span class="number">3</span>],<span class="number">10</span>,<span class="number">20</span>)</span><br><span class="line">        answer2.append(answer)</span><br><span class="line">    answer2 = np.array(answer2)         <span class="comment">#将list转换为numpy数组</span></span><br><span class="line">    answer3 = temp2(answer2)            <span class="comment">#数组正向化</span></span><br><span class="line">    answer4 = temp3(answer3)            <span class="comment">#标准化处理去钢</span></span><br><span class="line">    data = pd.DataFrame(answer4)        <span class="comment">#计算得分</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#将得分输出到excel表格中</span></span><br><span class="line">    writer = pd.ExcelWriter(<span class="string">&#x27;A.xlsx&#x27;</span>)		<span class="comment"># 写入Excel文件</span></span><br><span class="line">    data.to_excel(writer, <span class="string">&#x27;page_1&#x27;</span>, float_format=<span class="string">&#x27;%.5f&#x27;</span>)		<span class="comment"># ‘page_1’是写入excel的sheet名</span></span><br><span class="line">    writer.save()</span><br><span class="line">    writer.close()</span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure>
<p>参考：</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_41799019/article/details/97611462">Python实现TOPSIS分析法（优劣解距离法）_phythontopsis分析_XHHP的博客-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/37738503">TOPSIS法(优劣解距离法)介绍及 python3 实现 - 知乎 (zhihu.com)</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://yan1205.top">Yan1205</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://yan1205.top/2023/08/10/%E6%95%B0%E6%A8%A1%E7%AE%97%E6%B3%95python%E6%A8%A1%E6%9D%BF/">http://yan1205.top/2023/08/10/%E6%95%B0%E6%A8%A1%E7%AE%97%E6%B3%95python%E6%A8%A1%E6%9D%BF/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yan1205.top" target="_blank">风雨天一阁📜</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.skypro.cartafi.cn/2023/04/24/644563850bd39.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/08/10/SE%E4%BB%A3%E7%A0%81/" title="SE代码"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.skypro.cartafi.cn/2023/04/24/6445634b1a677.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">SE代码</div></div></a></div><div class="next-post pull-right"><a href="/2023/08/03/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/" title="数据预处理"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.skypro.cartafi.cn/2023/04/24/6445632ecdd62.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">数据预处理</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/head3.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Yan1205</div><div class="author-info__description">分享学习、极客、悟道，并记录我所热爱的生活。</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">19</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Yan1205"><i class="fab fa-github"></i><span>前往小家👣...</span></a><div class="card-info-social-icons is-center"><a class="social-icon faa-parent animated-hover" href="/asserts/weixinQRcode.png" target="_blank" title="微信"><svg class="social_icon faa-tada" aria-hidden="true"><use xlink:href="#icon-weixin-01"></use></svg></a><a class="social-icon faa-parent animated-hover" href="https://space.bilibili.com/39641431" target="_blank" title="B站"><svg class="social_icon faa-tada" aria-hidden="true"><use xlink:href="#icon-bilibili"></use></svg></a><a class="social-icon faa-parent animated-hover" href="mailto:yan1205.1875170531@gmail.com" target="_blank" title="Gmail"><svg class="social_icon faa-tada" aria-hidden="true"><use xlink:href="#icon-gmail"></use></svg></a><a class="social-icon faa-parent animated-hover" href="https://github.com/Yan1205" target="_blank" title="GitHub"><svg class="social_icon faa-tada" aria-hidden="true"><use xlink:href="#icon-GitHub"></use></svg></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到我的空间</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E8%A7%84%E5%88%92%E9%97%AE%E9%A2%98"><span class="toc-text">一、规划问题</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92"><span class="toc-text">(1)线性规划</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E7%9B%B8%E5%85%B3%E6%80%A7%E5%88%86%E6%9E%90"><span class="toc-text">二、相关性分析</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#t-%E6%A3%80%E9%AA%8C%EF%BC%9A%E6%A3%80%E9%AA%8C%E4%B8%A4%E4%B8%AA%E5%8F%98%E9%87%8F%E6%98%AF%E5%90%A6%E5%AD%98%E5%9C%A8%E5%B7%AE%E5%BC%82"><span class="toc-text">t-检验：检验两个变量是否存在差异</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%92%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="toc-text">三、数据处理和机器学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5-%E4%B8%89%E7%A7%8D%E6%96%B9%E5%BC%8F"><span class="toc-text">数据导入#三种方式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E7%90%86%E8%A7%A3"><span class="toc-text">数据理解</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-text">数据可视化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-%E5%B0%86%E6%95%B0%E6%8D%AE%E8%BD%AC%E6%8D%A2%E5%88%B0-0-1"><span class="toc-text">数据预处理:将数据转换到[0,1].</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%91%A0%E6%95%B0%E6%8D%AE%E7%BC%A9%E6%94%BE%EF%BC%88-MinMaxScaler-%EF%BC%89"><span class="toc-text">①数据缩放（ MinMaxScaler ）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%91%A1%E6%AD%A3%E6%80%81%E5%8C%96%E6%95%B0%E6%8D%AE%EF%BC%88StandardScaler%EF%BC%89"><span class="toc-text">②正态化数据（StandardScaler）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%91%A2%E6%A0%87%E5%87%86%E5%8C%96%E6%95%B0%E6%8D%AE%EF%BC%88Normalizer%EF%BC%89%E5%B8%B8%E7%94%A8"><span class="toc-text">③标准化数据（Normalizer）常用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%91%A3%E4%BA%8C%E5%80%BC%E6%95%B0%E6%8D%AE%EF%BC%88Binarizer%EF%BC%89"><span class="toc-text">④二值数据（Binarizer）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E7%89%B9%E5%BE%81%E9%80%89%E5%AE%9A"><span class="toc-text">数据特征选定</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%91%A0%E5%8D%95%E5%8F%98%E9%87%8F%E7%89%B9%E5%BE%81%E9%80%89%E5%AE%9A-SelectKBest%E7%B1%BB"><span class="toc-text">①单变量特征选定(SelectKBest类)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%91%A1%E9%80%92%E5%BD%92%E7%89%B9%E5%BE%81%E6%B6%88%E9%99%A4-%E5%B8%B8%E7%94%A8"><span class="toc-text">②递归特征消除    常用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%91%A2%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90"><span class="toc-text">③主成分分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%91%A3%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7"><span class="toc-text">④特征重要性</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E6%A8%A1%E5%9E%8B"><span class="toc-text">四、模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E8%AF%84%E4%BC%B0%E7%AE%97%E6%B3%95"><span class="toc-text">(1)评估算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%91%A0%E5%88%86%E7%A6%BB%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E8%AF%84%E4%BC%B0%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">①分离数据集和评估数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%91%A1k%E6%8A%98%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E5%88%86%E7%A6%BB"><span class="toc-text">②k折交叉验证分离</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%91%A2%E5%BC%83%E4%B8%80%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E5%88%86%E7%A6%BB"><span class="toc-text">③弃一交叉验证分离.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%91%A3%E9%87%8D%E5%A4%8D%E9%9A%8F%E6%9C%BA%E8%AF%84%E4%BC%B0%E3%80%81%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%86%E7%A6%BB%E3%80%82"><span class="toc-text">④重复随机评估、训练数据集分离。</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E7%AE%97%E6%B3%95%E8%AF%84%E4%BC%B0"><span class="toc-text">(2)算法评估</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%91%A0%E5%88%86%E7%B1%BB%E5%87%86%E7%A1%AE%E5%BA%A6"><span class="toc-text">①分类准确度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%91%A1%E5%AF%B9%E6%95%B0%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">②对数损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%91%A3AUC%E5%9B%BE"><span class="toc-text">④AUC图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%91%A4%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5"><span class="toc-text">⑤混淆矩阵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%91%A5%E7%BB%93%E6%9E%9C%E6%8A%A5%E5%91%8A"><span class="toc-text">⑥结果报告</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%91%A6%E7%BB%9D%E5%AF%B9%E5%9D%87%E5%80%BC%E8%AF%AF%E5%B7%AE"><span class="toc-text">⑦绝对均值误差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%91%A7%E5%86%B3%E5%AE%9A%E7%B3%BB%E6%95%B0%EF%BC%88-%F0%9D%91%85-2-%EF%BC%89"><span class="toc-text">⑧决定系数（$𝑅^2$）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%91%A1%E6%8F%90%E5%8D%87-Boosting-%E7%AE%97%E6%B3%95"><span class="toc-text">②提升(Boosting)算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%91%A2%E6%8A%95%E7%A5%A8-Voting-%E7%AE%97%E6%B3%95"><span class="toc-text">③投票(Voting)算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E8%B0%83%E5%8F%82"><span class="toc-text">(5)调参</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%91%A0%E7%BD%91%E7%BB%9C%E6%90%9C%E7%B4%A2%E4%BC%98%E5%8C%96%E8%B0%83%E5%8F%82"><span class="toc-text">①网络搜索优化调参</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%91%A1%E9%9A%8F%E6%9C%BA%E6%90%9C%E7%B4%A2%E4%BC%98%E5%8C%96%E8%B0%83%E5%8F%82"><span class="toc-text">②随机搜索优化调参</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E5%AE%9E%E6%88%98"><span class="toc-text">(6)实战</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E7%BB%98%E5%9B%BE"><span class="toc-text">五、绘图</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E7%AE%B1%E5%9E%8B%E5%9B%BE"><span class="toc-text">(1)箱型图</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%AD%A3%E5%A4%AA%E5%88%86%E5%B8%83%E5%9B%BE"><span class="toc-text">(2)正太分布图</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AD%E3%80%81%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%88%86%E6%9E%90"><span class="toc-text">六、聚类分析(无监督分析)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-k-means"><span class="toc-text">(1)k-means</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%83%E3%80%81%E8%AF%84%E4%BB%B7%E6%A8%A1%E5%9E%8B"><span class="toc-text">七、评价模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Topsis"><span class="toc-text">(1)Topsis</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 By Yan1205</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"></div><canvas id="universe"></canvas><script defer src="/js/universe.js"></script><script defer src="/js/cursor.js"></script><div class="aplayer no-destroy" data-id="8056800527" data-server="netease" data-type="playlist"   data-order="list" data-fixed="true" data-preload="auto" data-autoplay="false" data-mutex="true" ></div><canvas id="snow"></canvas><script async src="/js/snow.js"></script><script async src="/js/fps.js"></script><script async src="//at.alicdn.com/t/c/font_3865131_23yk0m7309o.js"></script><script defer src="https://npm.elemecdn.com/jquery@latest/dist/jquery.min.js"></script><script defer data-pjax src="/js/cat.js"></script><script defer data-pjax src="/js/readPercent.js"></script><script src="/js/runtime.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="http://yan1205.top/categories/算法/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 小冰の魔改教程 (5)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item" style="visibility: hidden"></div><a class="magnet_link_more"  href="http://yan1205.top/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style><div class="js-pjax"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '2s');
    arr[i].setAttribute('data-wow-delay', '1s');
    arr[i].setAttribute('data-wow-offset', '100');
    arr[i].setAttribute('data-wow-iteration', '2');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script></div><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow_init.js"></script><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script><!-- hexo injector body_end end --></body></html>